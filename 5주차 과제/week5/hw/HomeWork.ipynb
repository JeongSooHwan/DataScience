{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "from pprint import pprint \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from data.make_casting_graph import oneway_to_bidirected_graph\n",
    "from scipy.sparse import csc_matrix\n",
    "import time\n",
    "from pagerank import pagerank\n",
    "from sklearn.preprocessing import normalize\n",
    "from pyvis.network import Network\n",
    "from pagerank import pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create idx to num comments\n",
    "with open('./data/ratings.csv', encoding='utf-8') as f:\n",
    "    docs = [line.strip().split(',') for line in f.readlines()[1:]]\n",
    "    _idx2numcomments = {movie_idx:int(num) for num, movie_idx in docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre defined casting weight graph\n",
    "with open('./data/casting_graph.pkl', 'rb') as f:\n",
    "    graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create idx to actor name function\n",
    "with open('./data/actors.csv', encoding='utf-8') as f:\n",
    "    next(f)\n",
    "    docs = [line.split(',') for line in f.readlines()[1:]]\n",
    "    # English name if exist else Korean name\n",
    "    _idx2actor = {doc[0]:doc[1] for doc in docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/movies.csv', encoding='utf-8') as f:\n",
    "    next(f)\n",
    "    docs = [line.split(',') for line in f.readlines()[1:]]\n",
    "    _idx2movie = {doc[0]:doc[1] for doc in docs if len(docs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2movie = lambda idx: _idx2movie.get(idx, 'Unknown')\n",
    "idx2actor = lambda idx: _idx2actor.get(idx, 'Unknown')\n",
    "idx2numcomments = lambda idx: _idx2numcomments.get(idx,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = oneway_to_bidirected_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기생충 40\n",
      "극한직업 15\n",
      "마약왕 15\n",
      "인터스텔라 14\n",
      "어벤져스: 엔드게임 12\n",
      "걸캅스 12\n",
      "마녀 12\n",
      "택시운전사 11\n",
      "배심원들 11\n",
      "신과함께-죄와 벌 11\n"
     ]
    }
   ],
   "source": [
    "for movie in sorted(_idx2numcomments.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(idx2movie(movie[0]), movie[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(casting_csv_path):\n",
    "    # load file\n",
    "    with open(casting_csv_path, encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        graph = {line.split('\\t')[0]:line.split('\\t')[1].strip().split() for line in f if len(line.split('\\t'))==2}\n",
    "    \n",
    "    # weighting\n",
    "    # casting order (n-i)^2/ sum (i^2 for i = 1 to n)\n",
    "    def weight(casting_order):\n",
    "        if not casting_order:\n",
    "            return {}\n",
    "        n = len(casting_order)\n",
    "        weights = [(n-i) ** 2 for i in range(n)]\n",
    "        sum_ = sum(weights)\n",
    "        return {actor:w/sum_ for actor, w in zip(casting_order, weights)}\n",
    "    \n",
    "    graph = {movie:weight(actors) for movie, actors in graph.items() if actors}\n",
    "    return graph\n",
    "\n",
    "def oneway_to_bidirected_graph(graph):\n",
    "    \"\"\"Input: graph[movie][actor] = weight graph\"\"\"\n",
    "    # bi-directed graph\n",
    "    # graph has only one-way link: movie -> actor\n",
    "    actor_weight_sum = {}\n",
    "\n",
    "    # cumulate actor weights\n",
    "    for movie, actors in graph.items():\n",
    "        for actor, weight in actors.items():\n",
    "            actor_weight_sum[actor] = actor_weight_sum.get(actor, 0) + weight\n",
    "\n",
    "    # make bi-directed graph\n",
    "    from collections import defaultdict\n",
    "    g = defaultdict(lambda: {})\n",
    "    for movie, actors in graph.items():\n",
    "        g['movie {}'.format(movie)] = {'actor {}'.format(a):w for a,w in actors.items()}\n",
    "        for actor, weight in actors.items():\n",
    "            g['actor {}'.format(actor)]['movie {}'.format(movie)] = weight / actor_weight_sum[actor]\n",
    "\n",
    "    g = dict(g)\n",
    "    return g\n",
    "\n",
    "def main():\n",
    "    casting_csv_path = './data/casting.txt'\n",
    "    graph_path = './data/casting_graph.pkl'\n",
    "\n",
    "    graph = make_graph(casting_csv_path)\n",
    "\n",
    "    import pickle\n",
    "    with open(graph_path, 'wb') as f:\n",
    "        pickle.dump(graph, f)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(G, bias=None, df=0.15,\n",
    "             max_iter=50, converge_error=0.001,verbose=0):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    G: Inbound graph, dict of dict\n",
    "        G[to_node][from_node] = weight (float)\n",
    "    df: damping factor, float. default 0.15\n",
    "    \"\"\"\n",
    "    \n",
    "    A, nodes_dict = _normalize(G)\n",
    "    N = len(nodes_dict) # number of nodes\n",
    "    sr = 1 - df # survival rate (1 -  damping factor)\n",
    "    ir = 1 / N # initial rank\n",
    "\n",
    "    # Initialization\n",
    "    rank_dict = {n:ir for n in nodes_dict}\n",
    "\n",
    "    # Initialization of bias\n",
    "    if not bias:\n",
    "        bias = {node:ir for node in nodes_dict}\n",
    "\n",
    "    # Iteration\n",
    "    for _iter in range(1, max_iter + 1):\n",
    "        rank_dict_new = {}\n",
    "\n",
    "        # t: to node, f: from node, w: weight\n",
    "        for t in nodes_dict:\n",
    "            f_dict = A.get(t, {})\n",
    "            rank_dict_t = sum((w*rank_dict[f] for f, w in f_dict.items())) if f_dict else 0\n",
    "            rank_dict_t = sr * rank_dict_t + df * bias.get(t, 0)\n",
    "            rank_dict_new[t] = rank_dict_t\n",
    "\n",
    "        # convergence check\n",
    "        diff = sum((abs(rank_dict[n] - rank_dict_new[n]) for n in nodes_dict))\n",
    "        if diff < converge_error:\n",
    "            if verbose:\n",
    "                print('Early stopped at iter = {}'.format(_iter))\n",
    "            break\n",
    "\n",
    "        if verbose:\n",
    "            sum_ = sum(rank_dict_new.values())\n",
    "            print('Iteration = {}, diff = {}, sum = {}'.format(_iter, diff, sum_))\n",
    "\n",
    "        rank_dict = rank_dict_new\n",
    "\n",
    "    return rank_dict\n",
    "\n",
    "\n",
    "def _normalize(G):\n",
    "    \"\"\"It returns outbound normalized graph\n",
    "    Arguments\n",
    "    ---------\n",
    "    G: inbound graph dict of dict\n",
    "    \"\"\"\n",
    "    # Sum of outbound weight\n",
    "    # t: to node, f: from node, w: weight\n",
    "    W_sum = {}    \n",
    "    for t, f_dict in G.items():\n",
    "        for f, w in f_dict.items():\n",
    "            W_sum[f] = W_sum.get(f, 0) + w\n",
    "    A = {t:{f:w/W_sum[f] for f,w in f_dict.items()} for t, f_dict in G.items()}    \n",
    "    nodes_dict = set(G.keys())\n",
    "    nodes_dict.update(W_sum)\n",
    "    return A, nodes_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [과제1] 성능비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 　>> Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1, diff = 0.6745935594038667, sum = 1.000000000000005\n",
      "Iteration = 2, diff = 0.5133755765513068, sum = 1.0000000000000064\n",
      "Iteration = 3, diff = 0.4070843471025288, sum = 1.0000000000000073\n",
      "Iteration = 4, diff = 0.3288114569044879, sum = 1.000000000000001\n",
      "Iteration = 5, diff = 0.269000062616973, sum = 1.0000000000000044\n",
      "Iteration = 6, diff = 0.2217292304456657, sum = 0.9999999999999902\n",
      "Iteration = 7, diff = 0.18372765496993057, sum = 0.9999999999999921\n",
      "Iteration = 8, diff = 0.15290648077655614, sum = 1.0000000000000033\n",
      "Iteration = 9, diff = 0.12756391624362184, sum = 0.9999999999999933\n",
      "Iteration = 10, diff = 0.10676563571706411, sum = 0.9999999999999923\n",
      "Iteration = 11, diff = 0.08947335545631485, sum = 1.0000000000000075\n",
      "Iteration = 12, diff = 0.07517014319662849, sum = 1.0000000000000107\n",
      "Iteration = 13, diff = 0.0631852881114481, sum = 0.9999999999999923\n",
      "Iteration = 14, diff = 0.05320609097840659, sum = 0.9999999999999906\n",
      "Iteration = 15, diff = 0.04483047792706704, sum = 1.0000000000000095\n",
      "Iteration = 16, diff = 0.0378108514646867, sum = 1.0000000000000073\n",
      "Iteration = 17, diff = 0.03191234729017601, sum = 1.000000000000004\n",
      "Iteration = 18, diff = 0.026947257238086504, sum = 1.000000000000003\n",
      "Iteration = 19, diff = 0.02277018406289624, sum = 1.0000000000000075\n",
      "Iteration = 20, diff = 0.01924796744658092, sum = 1.000000000000001\n",
      "Iteration = 21, diff = 0.016277037274941425, sum = 1.0000000000000024\n",
      "Iteration = 22, diff = 0.013770288868780641, sum = 0.9999999999999968\n",
      "Iteration = 23, diff = 0.011652758751255288, sum = 0.9999999999999958\n",
      "Iteration = 24, diff = 0.009864207584992398, sum = 1.0000000000000095\n",
      "Iteration = 25, diff = 0.008351860248972012, sum = 1.0000000000000018\n",
      "Iteration = 26, diff = 0.007073923041220957, sum = 0.9999999999999977\n",
      "Iteration = 27, diff = 0.005992222715457387, sum = 1.0000000000000029\n",
      "Iteration = 28, diff = 0.00507729424217711, sum = 0.9999999999999981\n",
      "Iteration = 29, diff = 0.004302804710051078, sum = 0.9999999999999963\n",
      "Iteration = 30, diff = 0.003646940923353475, sum = 0.9999999999999943\n",
      "computation time : 0.31s\n"
     ]
    }
   ],
   "source": [
    "bias = {node:(idx2numcomments(node.split()[1]) if node[0] == 'm' else 0) for node in g}\n",
    "_sum = sum(bias.values())\n",
    "bias = {node:b / _sum for node, b in bias.items()}\n",
    "\n",
    "starttime = time.time()\n",
    "rank_dict = pagerank(g,\n",
    "               bias = bias,\n",
    "               df = 0.15,\n",
    "               max_iter = 30,\n",
    "               converge_error = 0.001,\n",
    "               verbose = 1)\n",
    "print('computation time : {0}s'.format(round(time.time()-starttime, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6154,)\n",
      "(6154, 6154)\n"
     ]
    }
   ],
   "source": [
    "nodes = set(g.keys())\n",
    "idx2node = list(sorted(nodes))\n",
    "node2idx = {node:idx for idx, node in enumerate(idx2node)}\n",
    "\n",
    "bias = np.asarray([b for node, b in sorted(bias.items(), key = lambda tp: node2idx[tp[0]])])\n",
    "print(bias.shape)\n",
    "\n",
    "rows = []\n",
    "cols = []\n",
    "data = []\n",
    "\n",
    "for from_node, to_dict in g.items():\n",
    "    from_idx = node2idx[from_node]\n",
    "    for to_node, weight in to_dict.items():\n",
    "        to_idx = node2idx[to_node]\n",
    "        rows.append(from_idx)\n",
    "        cols.append(to_idx)\n",
    "        data.append(weight)\n",
    "\n",
    "A = csc_matrix((data, (rows, cols)))\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 　>> Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 : diff = 0.1685245368865779\n",
      "iter 2 : diff = 0.123534416788289\n",
      "iter 3 : diff = 0.11717242074154521\n",
      "iter 4 : diff = 0.08676250638774644\n",
      "iter 5 : diff = 0.08106650827175174\n",
      "iter 6 : diff = 0.06044614044638538\n",
      "iter 7 : diff = 0.05589952786903922\n",
      "iter 8 : diff = 0.04188475454126574\n",
      "iter 9 : diff = 0.038452782327255894\n",
      "iter 10 : diff = 0.0289095171904886\n",
      "iter 11 : diff = 0.026405522194198443\n",
      "iter 12 : diff = 0.01994486388644759\n",
      "iter 13 : diff = 0.01811137289916391\n",
      "iter 14 : diff = 0.013753287448751986\n",
      "iter 15 : diff = 0.012408911428306675\n",
      "iter 16 : diff = 0.009469243738374537\n",
      "iter 17 : diff = 0.008494000468005527\n",
      "iter 18 : diff = 0.006511648928942716\n",
      "iter 19 : diff = 0.005809774127703195\n",
      "iter 20 : diff = 0.004473307017566352\n",
      "iter 21 : diff = 0.0039712967053357525\n",
      "iter 22 : diff = 0.0030704578506105173\n",
      "iter 23 : diff = 0.0027152845982687866\n",
      "iter 24 : diff = 0.002106149459828414\n",
      "iter 25 : diff = 0.0018577039374234091\n",
      "iter 26 : diff = 0.0014438021951808503\n",
      "iter 27 : diff = 0.0012704561426783514\n",
      "iter 28 : diff = 0.0009892576559651914\n",
      "iter 29 : diff = 0.0008685536766681317\n",
      "iter 30 : diff = 0.000677506241060136\n",
      "computation time : 0.011s\n"
     ]
    }
   ],
   "source": [
    "starttime2 = time.time()\n",
    "max_iter = 30\n",
    "df = 0.85\n",
    "ir = 1 / A.shape[0]\n",
    "rank = np.asarray([ir] * A.shape[0])\n",
    "for n_iter in range(1, max_iter + 1):\n",
    "    rank_new = A.dot(rank)\n",
    "    rank_new = normalize(rank_new.reshape(1, -1), norm = 'l1').reshape(-1)\n",
    "    rank_new = df * rank_new + (1 - df) * bias\n",
    "    diff = abs(rank - rank_new).sum()\n",
    "    rank = rank_new\n",
    "    print('iter {} : diff = {}'.format(n_iter, diff))\n",
    "print('computation time : {0}s'.format(round(time.time()-starttime2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [과제2] 영화 Top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 　>> Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161967 , 기생충 , 0.0032033878121671224\n",
      "167651 , 극한직업 , 0.0014303471787626468\n",
      "175322 , 마녀 , 0.0011565783119412997\n",
      "156464 , 보헤미안 랩소디 , 0.0011527961465662747\n",
      "130966 , 부산행 , 0.001098819013448319\n",
      "177483 , 배심원들 , 0.0009469824923736168\n",
      "174065 , 걸캅스 , 0.0009354687095915042\n",
      "37886 , 클레멘타인 , 0.000918249213245038\n",
      "154449 , 리틀 포레스트 , 0.00091821747845663\n",
      "163788 , 알라딘 , 0.0007997936563664337\n"
     ]
    }
   ],
   "source": [
    "movie_rank = {node:rank for node, rank in rank_dict.items() if node[0] == 'm'}\n",
    "\n",
    "for movie_number, rating_rank in sorted(movie_rank.items(), key=lambda x:-x[1])[:10]:\n",
    "    movie_idx = movie_number.split()[1]\n",
    "    print(movie_number[6:],',',idx2movie(movie_idx),',',rating_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 　>> Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161967 , 기생충 , 0.0015437432925532173\n",
      "156464 , 보헤미안 랩소디 , 0.0010864984266341052\n",
      "175322 , 마녀 , 0.0008946794759721638\n",
      "174065 , 걸캅스 , 0.0008564445054703045\n",
      "167651 , 극한직업 , 0.0007648489380972874\n",
      "37886 , 클레멘타인 , 0.000728929546919159\n",
      "157297 , 마약왕 , 0.0007133104346250872\n",
      "71509 , 아저씨 , 0.0006938076365826392\n",
      "136900 , 어벤져스: 엔드게임 , 0.0006567566198412949\n",
      "163788 , 알라딘 , 0.000638759850450271\n"
     ]
    }
   ],
   "source": [
    "rank_ = {idx2node[idx]:value for idx, value in enumerate(rank)}\n",
    "movie_rank2 = {node:value for node, value in rank_.items() if 'movie' in node}\n",
    "\n",
    "for movie_number, rating_rank in sorted(movie_rank2.items(), key=lambda x:-x[1])[:10]:\n",
    "    movie_idx = movie_number.split()[1]\n",
    "    print(movie_number[6:],',',idx2movie(movie_idx),',',rating_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [과제3] 영화 Top 노드 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 　>> Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "import pandas as pd\n",
    "\n",
    "got_net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "\n",
    "# set the physics layout of the network\n",
    "got_net.barnes_hut()\n",
    "\n",
    "for movie_number,_ in sorted(movie_rank.items(), key=lambda x:-x[1])[:10]:\n",
    "    movie_idx = movie_number.split()[1] #top10 영화 Id \n",
    "    #top10 영화 이름으로 노드 생성\n",
    "    got_net.add_node(idx2movie(movie_idx), label='Top10영화: ' + idx2movie(movie_idx)) \n",
    "    #top10 영화에 출연하는 배우들로 노드를 생성하고 edge 연결\n",
    "    for actorNumber in g[movie_number]:   \n",
    "        got_net.add_node(actorNumber, label='배우이름: ' + _idx2actor[actorNumber[6:]])\n",
    "        got_net.add_edge(idx2movie(movie_idx), actorNumber)\n",
    "        #top10 영화에 출연하는 배우들이 출연하는 다른 영화들의 노드를 생성하고 edge 연결\n",
    "        for movieName in g.get(actorNumber):\n",
    "            #top10에 존재하는 영화들의 중복을 제거\n",
    "            if not movie_idx in movieName:\n",
    "                got_net.add_node(movieName, label='영화: ' + _idx2movie[movieName[6:]])\n",
    "                got_net.add_edge(actorNumber, movieName)\n",
    "\n",
    "neighbor_map = got_net.get_adj_list()\n",
    "\n",
    "got_net.show(\"DictionaryGraph.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 　>> Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "got_net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "\n",
    "# set the physics layout of the network\n",
    "got_net.barnes_hut()\n",
    "\n",
    "for movie_number,_ in sorted(movie_rank2.items(), key=lambda x:-x[1])[:10]:\n",
    "    movie_idx = movie_number.split()[1] #top10 영화 Id \n",
    "    #top10 영화 이름으로 노드 생성\n",
    "    got_net.add_node(idx2movie(movie_idx), label='Top10영화: '+idx2movie(movie_idx)) \n",
    "    #top10 영화에 출연하는 배우들로 노드를 생성하고 edge 연결\n",
    "    for actorNumber in g[movie_number]:   \n",
    "        got_net.add_node(actorNumber, label='배우이름: ' + _idx2actor[actorNumber[6:]])\n",
    "        got_net.add_edge(idx2movie(movie_idx), actorNumber)\n",
    "        #top10 영화에 출연하는 배우들이 출연하는 다른 영화들의 노드를 생성하고 edge 연결\n",
    "        for movieName in g.get(actorNumber):\n",
    "            #top10에 존재하는 영화들의 중복을 제거\n",
    "            if not movie_idx in movieName:\n",
    "                got_net.add_node(movieName, label='영화: ' + _idx2movie[movieName[6:]])\n",
    "                got_net.add_edge(actorNumber, movieName)\n",
    "\n",
    "neighbor_map = got_net.get_adj_list()\n",
    "\n",
    "got_net.show(\"NumpyGraph.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
