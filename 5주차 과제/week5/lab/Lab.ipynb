{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "from pprint import pprint \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from data.make_casting_graph import oneway_to_bidirected_graph\n",
    "from scipy.sparse import csc_matrix\n",
    "import time\n",
    "from pagerank import pagerank\n",
    "from sklearn.preprocessing import normalize\n",
    "from pyvis.network import Network\n",
    "from pagerank import pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create idx to num comments\n",
    "with open('./data/ratings.csv', encoding='utf-8') as f:\n",
    "    docs = [line.strip().split(',') for line in f.readlines()[1:]]\n",
    "    _idx2numcomments = {movie_idx:int(num) for num, movie_idx in docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre defined casting weight graph\n",
    "with open('./data/casting_graph.pkl', 'rb') as f:\n",
    "    graph = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create idx to actor name function\n",
    "with open('./data/actors.csv', encoding='utf-8') as f:\n",
    "    next(f)\n",
    "    docs = [line.split(',') for line in f.readlines()[1:]]\n",
    "    # English name if exist else Korean name\n",
    "    _idx2actor = {doc[0]:doc[1] for doc in docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/movies.csv', encoding='utf-8') as f:\n",
    "    next(f)\n",
    "    docs = [line.split(',') for line in f.readlines()[1:]]\n",
    "    _idx2movie = {doc[0]:doc[1] for doc in docs if len(docs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2movie = lambda idx: _idx2movie.get(idx, 'Unknown')\n",
    "idx2actor = lambda idx: _idx2actor.get(idx, 'Unknown')\n",
    "idx2numcomments = lambda idx: _idx2numcomments.get(idx,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = oneway_to_bidirected_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습1] 리뷰가 많은 영화 순위"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기생충 40\n",
      "극한직업 15\n",
      "마약왕 15\n",
      "인터스텔라 14\n",
      "어벤져스: 엔드게임 12\n",
      "걸캅스 12\n",
      "마녀 12\n",
      "택시운전사 11\n",
      "배심원들 11\n",
      "신과함께-죄와 벌 11\n"
     ]
    }
   ],
   "source": [
    "for movie in sorted(_idx2numcomments.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(idx2movie(movie[0]), movie[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습2] Dict를 이용한 PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(casting_csv_path):\n",
    "    # load file\n",
    "    with open(casting_csv_path, encoding='utf-8') as f:\n",
    "        next(f)\n",
    "        graph = {line.split('\\t')[0]:line.split('\\t')[1].strip().split() for line in f if len(line.split('\\t'))==2}\n",
    "    \n",
    "    # weighting\n",
    "    # casting order (n-i)^2/ sum (i^2 for i = 1 to n)\n",
    "    def weight(casting_order):\n",
    "        if not casting_order:\n",
    "            return {}\n",
    "        n = len(casting_order)\n",
    "        weights = [(n-i) ** 2 for i in range(n)]\n",
    "        sum_ = sum(weights)\n",
    "        return {actor:w/sum_ for actor, w in zip(casting_order, weights)}\n",
    "    \n",
    "    graph = {movie:weight(actors) for movie, actors in graph.items() if actors}\n",
    "    return graph\n",
    "\n",
    "def oneway_to_bidirected_graph(graph):\n",
    "    \"\"\"Input: graph[movie][actor] = weight graph\"\"\"\n",
    "    # bi-directed graph\n",
    "    # graph has only one-way link: movie -> actor\n",
    "    actor_weight_sum = {}\n",
    "\n",
    "    # cumulate actor weights\n",
    "    for movie, actors in graph.items():\n",
    "        for actor, weight in actors.items():\n",
    "            actor_weight_sum[actor] = actor_weight_sum.get(actor, 0) + weight\n",
    "\n",
    "    # make bi-directed graph\n",
    "    from collections import defaultdict\n",
    "    g = defaultdict(lambda: {})\n",
    "    for movie, actors in graph.items():\n",
    "        g['movie {}'.format(movie)] = {'actor {}'.format(a):w for a,w in actors.items()}\n",
    "        for actor, weight in actors.items():\n",
    "            g['actor {}'.format(actor)]['movie {}'.format(movie)] = weight / actor_weight_sum[actor]\n",
    "\n",
    "    g = dict(g)\n",
    "    return g\n",
    "\n",
    "def main():\n",
    "    casting_csv_path = './data/casting.txt'\n",
    "    graph_path = './data/casting_graph.pkl'\n",
    "\n",
    "    graph = make_graph(casting_csv_path)\n",
    "\n",
    "    import pickle\n",
    "    with open(graph_path, 'wb') as f:\n",
    "        pickle.dump(graph, f)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(G, bias=None, df=0.15,\n",
    "             max_iter=50, converge_error=0.001,verbose=0):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    G: Inbound graph, dict of dict\n",
    "        G[to_node][from_node] = weight (float)\n",
    "    df: damping factor, float. default 0.15\n",
    "    \"\"\"\n",
    "    \n",
    "    A, nodes_dict = _normalize(G)\n",
    "    N = len(nodes_dict) # number of nodes\n",
    "    sr = 1 - df # survival rate (1 -  damping factor)\n",
    "    ir = 1 / N # initial rank\n",
    "\n",
    "    # Initialization\n",
    "    rank_dict = {n:ir for n in nodes_dict}\n",
    "\n",
    "    # Initialization of bias\n",
    "    if not bias:\n",
    "        bias = {node:ir for node in nodes_dict}\n",
    "\n",
    "    # Iteration\n",
    "    for _iter in range(1, max_iter + 1):\n",
    "        rank_dict_new = {}\n",
    "\n",
    "        # t: to node, f: from node, w: weight\n",
    "        for t in nodes_dict:\n",
    "            f_dict = A.get(t, {})\n",
    "            rank_dict_t = sum((w*rank_dict[f] for f, w in f_dict.items())) if f_dict else 0\n",
    "            rank_dict_t = sr * rank_dict_t + df * bias.get(t, 0)\n",
    "            rank_dict_new[t] = rank_dict_t\n",
    "\n",
    "        # convergence check\n",
    "        diff = sum((abs(rank_dict[n] - rank_dict_new[n]) for n in nodes_dict))\n",
    "        if diff < converge_error:\n",
    "            if verbose:\n",
    "                print('Early stopped at iter = {}'.format(_iter))\n",
    "            break\n",
    "\n",
    "        if verbose:\n",
    "            sum_ = sum(rank_dict_new.values())\n",
    "            print('Iteration = {}, diff = {}, sum = {}'.format(_iter, diff, sum_))\n",
    "\n",
    "        rank_dict = rank_dict_new\n",
    "\n",
    "    return rank_dict\n",
    "\n",
    "\n",
    "def _normalize(G):\n",
    "    \"\"\"It returns outbound normalized graph\n",
    "    Arguments\n",
    "    ---------\n",
    "    G: inbound graph dict of dict\n",
    "    \"\"\"\n",
    "    # Sum of outbound weight\n",
    "    # t: to node, f: from node, w: weight\n",
    "    W_sum = {}    \n",
    "    for t, f_dict in G.items():\n",
    "        for f, w in f_dict.items():\n",
    "            W_sum[f] = W_sum.get(f, 0) + w\n",
    "    A = {t:{f:w/W_sum[f] for f,w in f_dict.items()} for t, f_dict in G.items()}    \n",
    "    nodes_dict = set(G.keys())\n",
    "    nodes_dict.update(W_sum)\n",
    "    return A, nodes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1, diff = 0.674593559403865, sum = 1.0000000000000044\n",
      "Iteration = 2, diff = 0.5133755765513087, sum = 1.000000000000009\n",
      "Iteration = 3, diff = 0.40708434710253083, sum = 1.0000000000000075\n",
      "Iteration = 4, diff = 0.3288114569044877, sum = 1.0000000000000036\n",
      "Iteration = 5, diff = 0.2690000626169722, sum = 1.000000000000007\n",
      "Iteration = 6, diff = 0.22172923044566514, sum = 0.9999999999999898\n",
      "Iteration = 7, diff = 0.18372765496993038, sum = 0.9999999999999948\n",
      "Iteration = 8, diff = 0.15290648077655603, sum = 1.0000000000000073\n",
      "Iteration = 9, diff = 0.12756391624362093, sum = 0.9999999999999956\n",
      "Iteration = 10, diff = 0.10676563571706428, sum = 0.9999999999999949\n",
      "Iteration = 11, diff = 0.08947335545631496, sum = 1.0000000000000067\n",
      "Iteration = 12, diff = 0.07517014319662851, sum = 1.0000000000000093\n",
      "Iteration = 13, diff = 0.06318528811144798, sum = 0.9999999999999916\n",
      "Iteration = 14, diff = 0.05320609097840669, sum = 0.9999999999999908\n",
      "Iteration = 15, diff = 0.044830477927067174, sum = 1.0000000000000029\n",
      "Iteration = 16, diff = 0.03781085146468667, sum = 1.0000000000000095\n",
      "Iteration = 17, diff = 0.031912347290175926, sum = 1.0\n",
      "Iteration = 18, diff = 0.026947257238086535, sum = 1.0000000000000029\n",
      "Iteration = 19, diff = 0.022770184062896118, sum = 1.0000000000000075\n",
      "Iteration = 20, diff = 0.019247967446580923, sum = 1.000000000000005\n",
      "Iteration = 21, diff = 0.01627703727494145, sum = 1.0000000000000004\n",
      "Iteration = 22, diff = 0.01377028886878067, sum = 0.9999999999999967\n",
      "Iteration = 23, diff = 0.011652758751255252, sum = 0.999999999999996\n",
      "Iteration = 24, diff = 0.00986420758499242, sum = 1.0000000000000058\n",
      "Iteration = 25, diff = 0.00835186024897199, sum = 1.0000000000000022\n",
      "Iteration = 26, diff = 0.007073923041220974, sum = 0.9999999999999956\n",
      "Iteration = 27, diff = 0.005992222715457375, sum = 1.0\n",
      "Iteration = 28, diff = 0.005077294242177105, sum = 0.9999999999999994\n",
      "Iteration = 29, diff = 0.004302804710051075, sum = 0.9999999999999993\n",
      "Iteration = 30, diff = 0.003646940923353488, sum = 0.9999999999999943\n"
     ]
    }
   ],
   "source": [
    "bias = {node:(idx2numcomments(node.split()[1]) if node[0] == 'm' else 0) for node in g}\n",
    "_sum = sum(bias.values())\n",
    "bias = {node:b / _sum for node, b in bias.items()}\n",
    "\n",
    "rank_dict = pagerank(g,\n",
    "               bias = bias,\n",
    "               df = 0.15,\n",
    "               max_iter = 30,\n",
    "               converge_error = 0.001,\n",
    "               verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습3] Numpy를 이용한 PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6154,)\n",
      "(6154, 6154)\n"
     ]
    }
   ],
   "source": [
    "nodes = set(g.keys())\n",
    "idx2node = list(sorted(nodes))\n",
    "node2idx = {node:idx for idx, node in enumerate(idx2node)}\n",
    "\n",
    "bias = np.asarray([b for node, b in sorted(bias.items(), key = lambda tp: node2idx[tp[0]])])\n",
    "print(bias.shape)\n",
    "\n",
    "rows = []\n",
    "cols = []\n",
    "data = []\n",
    "\n",
    "for from_node, to_dict in g.items():\n",
    "    from_idx = node2idx[from_node]\n",
    "    for to_node, weight in to_dict.items():\n",
    "        to_idx = node2idx[to_node]\n",
    "        rows.append(from_idx)\n",
    "        cols.append(to_idx)\n",
    "        data.append(weight)\n",
    "\n",
    "A = csc_matrix((data, (rows, cols)))\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 : diff = 0.1685245368865779\n",
      "iter 2 : diff = 0.123534416788289\n",
      "iter 3 : diff = 0.11717242074154521\n",
      "iter 4 : diff = 0.08676250638774644\n",
      "iter 5 : diff = 0.08106650827175174\n",
      "iter 6 : diff = 0.06044614044638538\n",
      "iter 7 : diff = 0.05589952786903922\n",
      "iter 8 : diff = 0.04188475454126574\n",
      "iter 9 : diff = 0.038452782327255894\n",
      "iter 10 : diff = 0.0289095171904886\n",
      "iter 11 : diff = 0.026405522194198443\n",
      "iter 12 : diff = 0.01994486388644759\n",
      "iter 13 : diff = 0.01811137289916391\n",
      "iter 14 : diff = 0.013753287448751986\n",
      "iter 15 : diff = 0.012408911428306675\n",
      "iter 16 : diff = 0.009469243738374537\n",
      "iter 17 : diff = 0.008494000468005527\n",
      "iter 18 : diff = 0.006511648928942716\n",
      "iter 19 : diff = 0.005809774127703195\n",
      "iter 20 : diff = 0.004473307017566352\n",
      "iter 21 : diff = 0.0039712967053357525\n",
      "iter 22 : diff = 0.0030704578506105173\n",
      "iter 23 : diff = 0.0027152845982687866\n",
      "iter 24 : diff = 0.002106149459828414\n",
      "iter 25 : diff = 0.0018577039374234091\n",
      "iter 26 : diff = 0.0014438021951808503\n",
      "iter 27 : diff = 0.0012704561426783514\n",
      "iter 28 : diff = 0.0009892576559651914\n",
      "iter 29 : diff = 0.0008685536766681317\n",
      "iter 30 : diff = 0.000677506241060136\n"
     ]
    }
   ],
   "source": [
    "max_iter = 30\n",
    "df = 0.85\n",
    "\n",
    "ir = 1 / A.shape[0]\n",
    "rank = np.asarray([ir] * A.shape[0])\n",
    "\n",
    "for n_iter in range(1, max_iter + 1):\n",
    "    rank_new = A.dot(rank)\n",
    "    rank_new = normalize(rank_new.reshape(1, -1), norm = 'l1').reshape(-1)\n",
    "    rank_new = df * rank_new + (1 - df) * bias\n",
    "    diff = abs(rank - rank_new).sum()\n",
    "    rank = rank_new\n",
    "    print('iter {} : diff = {}'.format(n_iter, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
